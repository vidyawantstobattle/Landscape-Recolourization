{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, UpSampling2D, Input\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'downloads/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 614 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "#resizing the images\n",
    "train = train_datagen.flow_from_directory(path, \n",
    "                                         target_size = (256,256),\n",
    "                                         batch_size = 128,\n",
    "                                         class_mode = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 256, 256, 1)\n",
      "(128, 256, 256, 2)\n"
     ]
    }
   ],
   "source": [
    "#converting the images from RGB format to Lab format\n",
    "\"\"\"\n",
    "iterate on each rgb image to convert it to lab format.\n",
    "assign L channel to X vector.\n",
    "assign A and B channels to Y vector.\n",
    "essentially, what we are doing is training model on 'L' values i.e black and white (or grayscale) values\n",
    "and associated A and B values. \n",
    "Thus, during testing, model should predict 'Y' vector based on input 'X' vector image (i.e a grayscale image) \n",
    "\"\"\"\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for img in train[0]:\n",
    "    try:\n",
    "        lab = rgb2lab(img)\n",
    "        X.append(lab[:,:,0])\n",
    "        Y.append(lab[:,:,1:] / 128) #A and B values range between -127 to 128, we divide by 128 for normalization.\n",
    "    except:\n",
    "        print(\"error\")\n",
    "        \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "#print(X.shape)\n",
    "#print(X)\n",
    "X = X.reshape(X.shape+(1,))\n",
    "print(X.shape)\n",
    "#print(X)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_39 (Conv2D)           (None, 128, 128, 64)      640       \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 32, 32, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 32, 32, 128)       295040    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2 (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 64, 64, 64)        73792     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_10 (UpSampling (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 128, 128, 32)      18464     \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 128, 128, 16)      4624      \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 128, 128, 2)       290       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_11 (UpSampling (None, 256, 256, 2)       0         \n",
      "=================================================================\n",
      "Total params: 6,219,410\n",
      "Trainable params: 6,219,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#encoder part\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3,3), activation = 'relu', padding = 'same',strides=2, input_shape = (256,256,1)))\n",
    "model.add(Conv2D(128, (3,3), activation = 'relu', padding = 'same'))\n",
    "model.add(Conv2D(128, (3,3), activation = 'relu', padding = 'same', strides=2))\n",
    "model.add(Conv2D(256, (3,3), activation = 'relu', padding = 'same'))\n",
    "model.add(Conv2D(256, (3,3), activation = 'relu', padding = 'same', strides=2))\n",
    "model.add(Conv2D(512, (3,3), activation = 'relu', padding = 'same'))\n",
    "model.add(Conv2D(512, (3,3), activation = 'relu', padding = 'same'))\n",
    "model.add(Conv2D(256, (3,3), activation = 'relu', padding = 'same'))\n",
    "\n",
    "#decoder part\n",
    "#NOTE:for last part used tanh because we  need to colourize image in this layer with 2 filters, A and B(i.e vector Y).\n",
    "#A and B have values ranging between -1 and 1. So tanh is used as its range is also between -1 and 1.\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation = 'relu', padding = 'same'))\n",
    "model.add(UpSampling2D((2,2)))\n",
    "model.add(Conv2D(64, (3,3), activation = 'relu', padding = 'same'))\n",
    "model.add(UpSampling2D((2,2)))\n",
    "model.add(Conv2D(32, (3,3), activation = 'relu', padding = 'same'))\n",
    "model.add(Conv2D(16, (3,3), activation = 'relu', padding = 'same'))\n",
    "model.add(Conv2D(2, (3,3), activation = 'tanh', padding = 'same'))\n",
    "model.add(UpSampling2D((2,2)))\n",
    "model.compile(optimizer = 'adam', loss = 'mse', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8/8 [==============================] - 71s 9s/step - loss: 0.0647 - accuracy: 0.5186 - val_loss: 0.0277 - val_accuracy: 0.5031\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 79s 10s/step - loss: 0.0278 - accuracy: 0.5925 - val_loss: 0.0259 - val_accuracy: 0.6944\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 81s 10s/step - loss: 0.0279 - accuracy: 0.5853 - val_loss: 0.0263 - val_accuracy: 0.6781\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 80s 10s/step - loss: 0.0284 - accuracy: 0.5743 - val_loss: 0.0256 - val_accuracy: 0.6927\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 82s 10s/step - loss: 0.0278 - accuracy: 0.6044 - val_loss: 0.0255 - val_accuracy: 0.6991\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 82s 10s/step - loss: 0.0276 - accuracy: 0.5950 - val_loss: 0.0252 - val_accuracy: 0.7090\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 91s 11s/step - loss: 0.0284 - accuracy: 0.5406 - val_loss: 0.0261 - val_accuracy: 0.5620\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 81s 10s/step - loss: 0.0279 - accuracy: 0.5782 - val_loss: 0.0248 - val_accuracy: 0.7124\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 82s 10s/step - loss: 0.0277 - accuracy: 0.6228 - val_loss: 0.0240 - val_accuracy: 0.7294\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 83s 10s/step - loss: 0.0271 - accuracy: 0.6140 - val_loss: 0.0232 - val_accuracy: 0.7310\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 82s 10s/step - loss: 0.0272 - accuracy: 0.6005 - val_loss: 0.0238 - val_accuracy: 0.7206\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 80s 10s/step - loss: 0.0271 - accuracy: 0.6370 - val_loss: 0.0235 - val_accuracy: 0.7269\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 75s 9s/step - loss: 0.0285 - accuracy: 0.5952 - val_loss: 0.0253 - val_accuracy: 0.7121\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 67s 8s/step - loss: 0.0274 - accuracy: 0.6139 - val_loss: 0.0252 - val_accuracy: 0.7024\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 65s 8s/step - loss: 0.0276 - accuracy: 0.5993 - val_loss: 0.0234 - val_accuracy: 0.7282\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 116s 14s/step - loss: 0.0279 - accuracy: 0.5926 - val_loss: 0.0250 - val_accuracy: 0.7032\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 88s 11s/step - loss: 0.0272 - accuracy: 0.6151 - val_loss: 0.0239 - val_accuracy: 0.7324\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 58s 7s/step - loss: 0.0280 - accuracy: 0.6014 - val_loss: 0.0230 - val_accuracy: 0.7269\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 60s 8s/step - loss: 0.0270 - accuracy: 0.6322 - val_loss: 0.0242 - val_accuracy: 0.7210\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 63s 8s/step - loss: 0.0267 - accuracy: 0.6527 - val_loss: 0.0234 - val_accuracy: 0.7242\n",
      "INFO:tensorflow:Assets written to: image_recolourization.model/assets\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y, validation_split = 0.1, epochs = 20, batch_size = 16)\n",
    "model.save('image_recolourization.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('image_recolourization.model', custom_objects = None, compile = True)\n",
    "img1_color = []\n",
    "\n",
    "img1 = img_to_array(load_img('test/test_image.jpeg'))\n",
    "img1 = resize(img1, (256,256))\n",
    "img1_color.append(img1)\n",
    "\n",
    "img1_color = np.array(img1_color, dtype = float)\n",
    "img1_color = rgb2lab(1.0/255*img1_color)[:,:,:,0]\n",
    "img1_color = img1_color.reshape(img1_color.shape+(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "output1 = model.predict(img1_color)\n",
    "output1 = output1*128\n",
    "\n",
    "result = np.zeros((256,256,3))\n",
    "result[:,:,0] = img1_color[0][:,:,0]\n",
    "result[:,:,1:] = output1[0]\n",
    "\n",
    "\n",
    "imsave(\"result.png\", lab2rgb(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
